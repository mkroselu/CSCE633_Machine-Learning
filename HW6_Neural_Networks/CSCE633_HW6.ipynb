{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68557c8",
   "metadata": {},
   "source": [
    "# HW 6: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0646cb",
   "metadata": {},
   "source": [
    "## Question 1: Machine learning for object recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3b249",
   "metadata": {},
   "source": [
    "In this problem, we will process images coming from the CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes (i.e., airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck) with 6000 images per class. There are 50000 training images and 10000 test images. More information about the data can be found here: https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "The CIFAR-10 dataset is included in most of the frameworks for deep neural networks e.g., Keras, Tensorflow, PyTorch. In Keras, CIFAR-10 can be downloaded by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c741e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9616c379",
   "metadata": {},
   "source": [
    "Note: The original CIFAR-10 with color images can take a long time to train. Hence, we will convert the color images into grayscale images. One way to achieve this would be to use a library such as scikit-image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9354e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "# convert to grayscale images\n",
    "X_train = rgb2gray(X_train)\n",
    "X_test = rgb2gray(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d1892",
   "metadata": {},
   "source": [
    "## (a) Visualization\n",
    "Randomly select and visualize 5-6 images (no need to include all the classes). \n",
    "Note: You can find a useful link on image pre-processing here: https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ef9750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b4d4bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Define the number of images to visualize\n",
    "num_images = 6\n",
    "\n",
    "# Select num_images random indices from the training set\n",
    "random_indices = np.random.choice(X_train.shape[0], num_images, replace=False)\n",
    "\n",
    "# Create a figure with num_images subplots\n",
    "fig, axes = plt.subplots(1, num_images, figsize=(12, 4))\n",
    "\n",
    "# Loop over the selected indices and plot the corresponding images\n",
    "for i, index in enumerate(random_indices):\n",
    "    # Get the image and its label\n",
    "    image = X_train[index]\n",
    "    label = y_train[index][0]\n",
    "    \n",
    "    # Set the title to the class name\n",
    "    title = class_names[label]\n",
    "    \n",
    "    # Plot the image\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc29ef7",
   "metadata": {},
   "source": [
    "## (b) Data exploration: \n",
    "Count the number of samples per class in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab3ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class airplane: 5000 samples\n",
      "Class automobile: 5000 samples\n",
      "Class bird: 5000 samples\n",
      "Class cat: 5000 samples\n",
      "Class deer: 5000 samples\n",
      "Class dog: 5000 samples\n",
      "Class frog: 5000 samples\n",
      "Class horse: 5000 samples\n",
      "Class ship: 5000 samples\n",
      "Class truck: 5000 samples\n"
     ]
    }
   ],
   "source": [
    "# Count the number of samples per class in the training set\n",
    "num_samples = []\n",
    "for i in range(len(class_names)):\n",
    "    num_samples.append((y_train == i).sum())\n",
    "\n",
    "# Print the results\n",
    "for i in range(len(class_names)):\n",
    "    print(\"Class {}: {} samples\".format(class_names[i], num_samples[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21091d4",
   "metadata": {},
   "source": [
    "## (c) Image classification with FNNs \n",
    "In this part, you will use a feedforward neural network (FNN) (also called \"multilayer perceptron\") to perform the object classification task. The input of the FNN comprises of all the pixels of the image. Use one of the five batches of the training\n",
    "data as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e1002f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X_train = X_train.astype('float32') / 255.0 # convert the pixel values to float32 and normalize them to the range [0, 1].\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "num_validation_samples = 10000\n",
    "X_val = X_train[:num_validation_samples]\n",
    "y_val = y_train[:num_validation_samples]\n",
    "X_train = X_train[num_validation_samples:]\n",
    "y_train = y_train[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98bd218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2186 - accuracy: 0.1843 - val_loss: 2.1447 - val_accuracy: 0.2120\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1170 - accuracy: 0.2322 - val_loss: 2.0896 - val_accuracy: 0.2388\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.0795 - accuracy: 0.2494 - val_loss: 2.0631 - val_accuracy: 0.2619\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.0597 - accuracy: 0.2627 - val_loss: 2.0474 - val_accuracy: 0.2717\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.0456 - accuracy: 0.2691 - val_loss: 2.0359 - val_accuracy: 0.2726\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.0330 - accuracy: 0.2783 - val_loss: 2.0239 - val_accuracy: 0.2742\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.0206 - accuracy: 0.2825 - val_loss: 2.0129 - val_accuracy: 0.2869\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.0077 - accuracy: 0.2883 - val_loss: 1.9982 - val_accuracy: 0.2891\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 1.9955 - accuracy: 0.2944 - val_loss: 1.9868 - val_accuracy: 0.2991\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 1.9835 - accuracy: 0.2989 - val_loss: 1.9755 - val_accuracy: 0.3052\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.9798 - accuracy: 0.3079\n",
      "Test accuracy: 0.30790001153945923\n"
     ]
    }
   ],
   "source": [
    "# Define the FNN model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660a0c3",
   "metadata": {},
   "source": [
    "## (c.i) \n",
    "Experiment on the validation set with different FNN hyper-parameters, e.g. # layers, # nodes per layer, activation function, dropout, weight regularization, etc. Choose 3 hyper-parameter combinations and for each combination, please do the following: (1) monitor the loss on the train and validation set across the epochs of the FNN training; (2) report the final classification accuracy on the training and validation sets; (3) report the running time for training the FNN; (4) report the # parameters that are learned for each FNN. \n",
    "Note: If running the FNN takes a long time, you can subsample the training data (i.e., choose a random set of samples from training) or sub-sample the input images to a smaller size (e.g., 24 x 24)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813b3ab",
   "metadata": {},
   "source": [
    "1st combination: add a hidden layer containing 128 nodes. Both validation accuracy (0.3302) and test accuracy (0.3345) increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "609caffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 4s 2ms/step - loss: 2.1613 - accuracy: 0.1941 - val_loss: 2.0768 - val_accuracy: 0.2415\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.0590 - accuracy: 0.2493 - val_loss: 2.0494 - val_accuracy: 0.2575\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.0315 - accuracy: 0.2632 - val_loss: 2.0120 - val_accuracy: 0.2771\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.0076 - accuracy: 0.2780 - val_loss: 2.0124 - val_accuracy: 0.2662\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.9849 - accuracy: 0.2896 - val_loss: 1.9664 - val_accuracy: 0.2975\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.9586 - accuracy: 0.2993 - val_loss: 1.9425 - val_accuracy: 0.3092\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.9316 - accuracy: 0.3097 - val_loss: 1.9258 - val_accuracy: 0.3137\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.9100 - accuracy: 0.3158 - val_loss: 1.8960 - val_accuracy: 0.3214\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.8921 - accuracy: 0.3233 - val_loss: 1.8854 - val_accuracy: 0.3233\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.8771 - accuracy: 0.3280 - val_loss: 1.8726 - val_accuracy: 0.3302\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.8721 - accuracy: 0.3345\n",
      "Test accuracy: 0.3345000147819519\n"
     ]
    }
   ],
   "source": [
    "# Define the FNN model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65506bd",
   "metadata": {},
   "source": [
    "2nd combination: increase the nodes of the hidden layers. Both validation accuracy (0.3737) and test accuracy (0.3698) increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e574df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 10s 7ms/step - loss: 2.1183 - accuracy: 0.2158 - val_loss: 2.0373 - val_accuracy: 0.2527\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0060 - accuracy: 0.2790 - val_loss: 1.9620 - val_accuracy: 0.2901\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.9380 - accuracy: 0.3062 - val_loss: 1.9156 - val_accuracy: 0.3100\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.8947 - accuracy: 0.3210 - val_loss: 1.8743 - val_accuracy: 0.3270\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.8658 - accuracy: 0.3303 - val_loss: 1.8495 - val_accuracy: 0.3382\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.8443 - accuracy: 0.3352 - val_loss: 1.8299 - val_accuracy: 0.3481\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.8203 - accuracy: 0.3487 - val_loss: 1.8138 - val_accuracy: 0.3542\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.8021 - accuracy: 0.3559 - val_loss: 1.8008 - val_accuracy: 0.3598\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.7843 - accuracy: 0.3620 - val_loss: 1.7917 - val_accuracy: 0.3639\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.7657 - accuracy: 0.3704 - val_loss: 1.7646 - val_accuracy: 0.3737\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.7653 - accuracy: 0.3699\n",
      "Test accuracy: 0.3698999881744385\n"
     ]
    }
   ],
   "source": [
    "# Define the FNN model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e343d",
   "metadata": {},
   "source": [
    "3rd combination: add a dropout rate of 0.5 in the hidden layers. Both validation accuracy (0.3358) and test accuracy (0.3366) decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b22fe176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 10s 7ms/step - loss: 2.1833 - accuracy: 0.1817 - val_loss: 2.0725 - val_accuracy: 0.2401\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0736 - accuracy: 0.2434 - val_loss: 2.0264 - val_accuracy: 0.2704\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0338 - accuracy: 0.2639 - val_loss: 1.9737 - val_accuracy: 0.2912\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 2.0050 - accuracy: 0.2760 - val_loss: 1.9493 - val_accuracy: 0.3031\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.9815 - accuracy: 0.2883 - val_loss: 1.9243 - val_accuracy: 0.3122\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 1.9660 - accuracy: 0.2925 - val_loss: 1.9034 - val_accuracy: 0.3218\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 1.9483 - accuracy: 0.2977 - val_loss: 1.8850 - val_accuracy: 0.3261\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 1.9378 - accuracy: 0.3027 - val_loss: 1.8774 - val_accuracy: 0.3268\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.9268 - accuracy: 0.3072 - val_loss: 1.8618 - val_accuracy: 0.3357\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.9165 - accuracy: 0.3097 - val_loss: 1.8611 - val_accuracy: 0.3358\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.8626 - accuracy: 0.3366\n",
      "Test accuracy: 0.33660000562667847\n"
     ]
    }
   ],
   "source": [
    "# Define the FNN model \n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971be6a",
   "metadata": {},
   "source": [
    "## (c.ii) \n",
    "Run the best model that was found based on the validation set from question (c.i) on the testing set. Report the classification accuracy on the testing set. Report the confusion matrix for each class.\n",
    "Note: The confusion matrix is a 10x10 matrix; its rows correspond to the actual labels for each class, while its columns correspond to the predicted classes. Element (i; j) includes the number of samples that belonged to the ith class and were predicted as the jth class. In a perfect classification task, the non-diagonal elements of the matrix will be all non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e71f664d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 10s 7ms/step - loss: 2.1269 - accuracy: 0.2110 - val_loss: 2.0451 - val_accuracy: 0.2658\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0288 - accuracy: 0.2674 - val_loss: 2.0114 - val_accuracy: 0.2715\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.9816 - accuracy: 0.2874 - val_loss: 1.9496 - val_accuracy: 0.2969\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.9297 - accuracy: 0.3077 - val_loss: 1.8969 - val_accuracy: 0.3210\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.8895 - accuracy: 0.3216 - val_loss: 1.8690 - val_accuracy: 0.3311\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.8660 - accuracy: 0.3306 - val_loss: 1.8538 - val_accuracy: 0.3401\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.8470 - accuracy: 0.3374 - val_loss: 1.8393 - val_accuracy: 0.3418\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 1.8331 - accuracy: 0.3431 - val_loss: 1.8392 - val_accuracy: 0.3441\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.8183 - accuracy: 0.3486 - val_loss: 1.8112 - val_accuracy: 0.3543\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.8020 - accuracy: 0.3554 - val_loss: 1.7928 - val_accuracy: 0.3624\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.7942 - accuracy: 0.3647\n",
      "\n",
      "\n",
      "Test accuracy: 0.36469998955726624\n"
     ]
    }
   ],
   "source": [
    "# Define the FNN model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('\\n')\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9630ca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "Confusion matrix:\n",
      "[[423  34 110  18  89   7  68  38 188  25]\n",
      " [ 79 395  12  21  39  11  80  24 194 145]\n",
      " [133  23 302  41 159  35 176  56  61  14]\n",
      " [ 97  27 101 135 136 113 192  69  81  49]\n",
      " [144  25 147  17 364  19 140  64  63  17]\n",
      " [109  13 115  93 119 250 117  60  96  28]\n",
      " [102  40  84  30 119  40 482  28  49  26]\n",
      " [156  21  82  37 125  43  72 347  68  49]\n",
      " [162  64  25  12  44  22  29  27 568  47]\n",
      " [ 82 149  20  29  22  10  74  50 184 380]]\n"
     ]
    }
   ],
   "source": [
    "# Generate the confusion matrix\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806fa003",
   "metadata": {},
   "source": [
    "## (d) Image classification with CNNs \n",
    "In this part, you will use a convolutional neural network (CNN) to perform the object classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52d8c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.astype(\"float32\") / 255.0 \n",
    "x_test = x_test.astype(\"float32\") / 255.0 \n",
    "y_train = tf.keras.utils.to_categorical(y_train) \n",
    "y_test = tf.keras.utils.to_categorical(y_test) \n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "num_validation_samples = 10000 \n",
    "x_val = x_train[:num_validation_samples] \n",
    "y_val = y_train[:num_validation_samples] \n",
    "x_train = x_train[num_validation_samples:] \n",
    "y_train = y_train[num_validation_samples:] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4c1c9",
   "metadata": {},
   "source": [
    "## (d.i) \n",
    "Experiment on the validation set with different CNN hyper-parameters, e.g. # layers, filter size, stride size, activation function, dropout, weight regularization, etc. Choose 3 hyper-parameter combinations and for each combination, please do the following: (1) monitor the loss on the train and validation set across the epochs of the CNN training; (2) report the final classification accuracy on the training and validation sets; (3) report the running time for training the CNN; (4) report the # parameters that are learned for each CNN. How do these metrics compare to the FNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c51d35",
   "metadata": {},
   "source": [
    "1st combination: base model. Both validation accuracy (0.6851) and test accuracy (0.69) increase comparing to the FNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c027f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 41s 31ms/step - loss: 1.5837 - accuracy: 0.4194 - val_loss: 1.3131 - val_accuracy: 0.5270\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 1.2169 - accuracy: 0.5670 - val_loss: 1.1708 - val_accuracy: 0.5913\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 1.0643 - accuracy: 0.6252 - val_loss: 1.0270 - val_accuracy: 0.6386\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.9612 - accuracy: 0.6607 - val_loss: 1.0080 - val_accuracy: 0.6526\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.8816 - accuracy: 0.6891 - val_loss: 0.9259 - val_accuracy: 0.6752\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.8174 - accuracy: 0.7116 - val_loss: 0.9537 - val_accuracy: 0.6655\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.7642 - accuracy: 0.7319 - val_loss: 0.9198 - val_accuracy: 0.6854\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.7177 - accuracy: 0.7472 - val_loss: 0.8925 - val_accuracy: 0.6957\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6747 - accuracy: 0.7641 - val_loss: 0.8837 - val_accuracy: 0.6975\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.6309 - accuracy: 0.7776 - val_loss: 0.9314 - val_accuracy: 0.6851\n",
      "313/313 - 4s - loss: 0.9373 - accuracy: 0.6900 - 4s/epoch - 12ms/step\n",
      "\n",
      "\n",
      "Test accuracy: 0.6899999976158142\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val)) \n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2) \n",
    "print('\\n')\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169e1e8",
   "metadata": {},
   "source": [
    "2nd combination: increase the nodes of one convolutional layer and one fully connected layer. Both validation accuracy (0.7002) and test accuracy (0.7017) increase comparing to base model of CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1442be2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 44s 33ms/step - loss: 1.5106 - accuracy: 0.4507 - val_loss: 1.3127 - val_accuracy: 0.5436\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.1494 - accuracy: 0.5913 - val_loss: 1.0499 - val_accuracy: 0.6259\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9841 - accuracy: 0.6535 - val_loss: 1.0074 - val_accuracy: 0.6432\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.8759 - accuracy: 0.6930 - val_loss: 0.9108 - val_accuracy: 0.6802\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 47s 38ms/step - loss: 0.7868 - accuracy: 0.7257 - val_loss: 0.8775 - val_accuracy: 0.7021\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 47s 38ms/step - loss: 0.7088 - accuracy: 0.7505 - val_loss: 0.8633 - val_accuracy: 0.7026\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 43s 35ms/step - loss: 0.6436 - accuracy: 0.7733 - val_loss: 0.8519 - val_accuracy: 0.7092\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 47s 37ms/step - loss: 0.5829 - accuracy: 0.7937 - val_loss: 0.8863 - val_accuracy: 0.7052\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 0.5221 - accuracy: 0.8151 - val_loss: 0.9203 - val_accuracy: 0.7073\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.4642 - accuracy: 0.8352 - val_loss: 0.9664 - val_accuracy: 0.7002\n",
      "313/313 - 4s - loss: 0.9835 - accuracy: 0.7017 - 4s/epoch - 12ms/step\n",
      "Test accuracy: 0.70169997215271\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a77a0",
   "metadata": {},
   "source": [
    "3rd combination: use the LeakyReLU activation function instead of relu. Both validation accuracy (0.7644) and test accuracy (0.7620) increase comparing to model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a3095a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 79s 61ms/step - loss: 1.4740 - accuracy: 0.5166 - val_loss: 1.1504 - val_accuracy: 0.6407\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 74s 59ms/step - loss: 1.0773 - accuracy: 0.6825 - val_loss: 1.0690 - val_accuracy: 0.6865\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 74s 59ms/step - loss: 0.9555 - accuracy: 0.7311 - val_loss: 1.0335 - val_accuracy: 0.7063\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.8811 - accuracy: 0.7655 - val_loss: 0.9631 - val_accuracy: 0.7410\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 74s 59ms/step - loss: 0.8351 - accuracy: 0.7869 - val_loss: 0.9518 - val_accuracy: 0.7487\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 75s 60ms/step - loss: 0.7882 - accuracy: 0.8084 - val_loss: 0.9667 - val_accuracy: 0.7547\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 75s 60ms/step - loss: 0.7478 - accuracy: 0.8252 - val_loss: 0.9367 - val_accuracy: 0.7666\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 73s 58ms/step - loss: 0.7166 - accuracy: 0.8401 - val_loss: 0.9776 - val_accuracy: 0.7679\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 72s 58ms/step - loss: 0.6932 - accuracy: 0.8520 - val_loss: 0.9896 - val_accuracy: 0.7614\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.6647 - accuracy: 0.8659 - val_loss: 1.0183 - val_accuracy: 0.7644\n",
      "313/313 - 6s - loss: 1.0387 - accuracy: 0.7620 - 6s/epoch - 19ms/step\n",
      "Test accuracy: 0.7620000243186951\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LeakyReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# The alpha parameter controls the slope of the negative part of the activation function\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', \n",
    "           input_shape=(32, 32, 3), activation=LeakyReLU(alpha=0.1)),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', \n",
    "           activation=LeakyReLU(alpha=0.1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', \n",
    "           activation=LeakyReLU(alpha=0.1)),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', \n",
    "           activation=LeakyReLU(alpha=0.1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation=LeakyReLU(alpha=0.1), \n",
    "          kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7a7b9",
   "metadata": {},
   "source": [
    "**Ans**: the training time of the CNN models (26-79secs) are all longer than FNN models (~10secs). The test and validation accuracy of all 3 CNN models are all higher than the FNN models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d254d5e",
   "metadata": {},
   "source": [
    "## (d.ii) \n",
    "Run the best model that was found based on the validation set from question (d.i) on the testing set. Report the classification accuracy on the testing set. How does this metric compare to the FNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c51a06",
   "metadata": {},
   "source": [
    "**Ans**: The test accuracy of the best model with CNN (0.7620) is higher than that of the best model with FNN (0.3647). The time used to train the CNN model is also longer than that of the FNN model. Both the loss on the train (0.6647) and validation (1.0183) set for CNN model are lower than the loss on the train (1.8020) and validation (1.7928) set for the FNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bcc4730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 4s - loss: 1.0387 - accuracy: 0.7620 - 4s/epoch - 12ms/step\n",
      "Test accuracy: 0.7620000243186951\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0eee31ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 13ms/step\n",
      "Confusion matrix:\n",
      "[[804  20  50   8  10   2   4  17  59  26]\n",
      " [ 17 890   6   2   1   6   1   2  20  55]\n",
      " [ 62   4 702  22  69  52  43  24  15   7]\n",
      " [ 35  10  79 445  73 207  67  47  21  16]\n",
      " [ 15   4  79  38 731  33  29  56  14   1]\n",
      " [ 14   5  64  78  41 696  18  64  11   9]\n",
      " [  8   5  43  25  32  18 841  11  14   3]\n",
      " [ 15   1  41  17  44  44   6 819   6   7]\n",
      " [ 38  20  14   5   5   2   3   4 885  24]\n",
      " [ 29  91  10   3   4   2   4  26  24 807]]\n"
     ]
    }
   ],
   "source": [
    "# Generate the confusion matrix\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59488baf",
   "metadata": {},
   "source": [
    "## (e) Bayesian optimization for hyper-parameter tuning\n",
    "Instead of performing grid or random search to tune the hyper-parameters of the CNN, we can also try a model-based method for finding the optimal hyper-parameters through Bayesian optimization. This method performs a more intelligent search on the hyperparameter space in order to estimate the best set of hyperparameters for the data. Use publicly available libraries (e.g.,\n",
    "hyperopt in Python) to perform a Bayesian optimization on the hyperparameter space using the validation set. Report the emotion classification accuracy on the testing set.\n",
    "Hint: Check this: https://github.com/hyperopt/hyperopt and this source: https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f73d29",
   "metadata": {},
   "source": [
    "CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f60d804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe, fmin, hp\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "93a66077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b7212a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 3)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.astype('float32') / 255.0 # convert the pixel values to float32 and normalize them to the range [0, 1].\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "num_validation_samples = 10000\n",
    "X_val = X_train[:num_validation_samples]\n",
    "y_val = y_train[:num_validation_samples]\n",
    "X_train = X_train[num_validation_samples:]\n",
    "y_train = y_train[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "43b08b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 data\n",
    "#(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "num_classes = 10\n",
    "\n",
    "# Preprocess the data\n",
    "#x_train = x_train.astype('float32') / 255.\n",
    "#x_test = x_test.astype('float32') / 255.\n",
    "#y_train = to_categorical(y_train, num_classes)\n",
    "#y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define the search space\n",
    "space = {'num_filters': hp.choice('num_filters', [32, 64, 128]),\n",
    "         'kernel_size': hp.choice('kernel_size', [3, 5, 7]),\n",
    "         'pool_size': hp.choice('pool_size', [2, 3]),\n",
    "         'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "         'learning_rate': hp.loguniform('learning_rate', -5, -1)}\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(params['num_filters'], (params['kernel_size'], params['kernel_size']), activation='relu', padding='same', input_shape=X_train.shape[1:]))\n",
    "    model.add(MaxPooling2D((params['pool_size'], params['pool_size'])))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Conv2D(params['num_filters'], (params['kernel_size'], params['kernel_size']), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((params['pool_size'], params['pool_size'])))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(lr=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=0, validation_data=(X_test, y_test))\n",
    "    run_time = time.time() - start_time\n",
    "\n",
    "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return {'loss': -accuracy, 'status': 'ok', 'params': params, 'run_time': run_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "307bfe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [1:38:17<00:00, 589.76s/trial, best loss: -0.4596000015735626]\n",
      "{'dropout': 0.15282827305034197, 'kernel_size': 1, 'learning_rate': 0.012384490053459417, 'num_filters': 2, 'pool_size': 1}\n"
     ]
    }
   ],
   "source": [
    "# Run the optimization\n",
    "best = fmin(objective, space, algo = tpe.suggest, max_evals=10)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6c3f7954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 1.8912 - accuracy: 0.3117 - val_loss: 1.7438 - val_accuracy: 0.3829\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.7165 - accuracy: 0.3873 - val_loss: 1.6288 - val_accuracy: 0.4303\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.6144 - accuracy: 0.4297 - val_loss: 1.5720 - val_accuracy: 0.4446\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.5464 - accuracy: 0.4553 - val_loss: 1.5028 - val_accuracy: 0.4674\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4984 - accuracy: 0.4701 - val_loss: 1.4747 - val_accuracy: 0.4786\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.4611 - accuracy: 0.4839 - val_loss: 1.4747 - val_accuracy: 0.4771\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 1.4293 - accuracy: 0.4931 - val_loss: 1.4471 - val_accuracy: 0.4927\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3986 - accuracy: 0.5054 - val_loss: 1.4545 - val_accuracy: 0.4837\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3678 - accuracy: 0.5166 - val_loss: 1.4378 - val_accuracy: 0.4930\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3436 - accuracy: 0.5252 - val_loss: 1.4659 - val_accuracy: 0.4874\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.4589 - accuracy: 0.4895\n",
      "Test accuracy: 0.4894999861717224\n"
     ]
    }
   ],
   "source": [
    "# Train the best model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(best['num_filters'], (best['kernel_size'], best['kernel_size']), activation='relu', padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling2D((best['pool_size'], best['pool_size'])))\n",
    "model.add(Dropout(best['dropout']))\n",
    "model.add(Conv2D(best['num_filters'], (best['kernel_size'], best['kernel_size']), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((best['pool_size'], best['pool_size'])))\n",
    "model.add(Dropout(best['dropout']))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(best['dropout']))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41b593",
   "metadata": {},
   "source": [
    "**Ans**: The test accuracy of the model with CNN is 0.4895. The time used to train the CNN model is about 17 secs per epoch. The loss on the train and validation set is 1.3436 and 1.4659 respectively. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
